# gpt3-contextual

Contextual chat with GPT-3 of OpenAI API.

# ğŸš€ Quick start

Install.

```bash
$ pip install gpt3-contextual
```

Make script as console.py.

```python
import asyncio
from gpt3contextual import ContextualChat

async def main():
    cc = ContextualChat(
        "YOUR_OPENAI_APIKEY",
        username="Human",
        agentname="AI"
    )

    while True:
        text = input("human> ")
        resp, _ = await cc.chat("user1234567890", text)
        print(f"AI> {resp}")

asyncio.run(main())
```

Run console.py and chat with GPT-3.

```
human> hello
AI>  Hi, how can I help you?
human> I'm hungry
AI>  What would you like to eat?
human> sandwitches
AI>  What kind of sandwich would you like?
human> ham&egg        
AI>  Would you like that on a white or wheat bread?
human> wheet
AI>  Would you like anything else with your ham and egg sandwich on wheat bread?
human> Everything is fine, thank you.
AI>  Great! Your order has been placed. Enjoy your meal!
```


# ğŸ§¸ Usage

You can set parameters to customize conversation senario when you make the instance of `ContextualChat`.
See also https://platform.openai.com/docs/api-reference/completions to understands some params.

- `api_key`: str : API key for OpenAI API.
- `context_count`: int : Count of turns(request and response) to use as the context. Default=`6`
- `username`: str : Name of roll of user. Default=`customer`.
- `agentname`: str = Name or role of agent(bot). Default=`agent`.
- `chat_description`: str : Some conditions to be considered in the senario of conversation.
- `engine`: str : The engine(model) to use. Default=`text-davinci-003`.
- `temperature`: float : What sampling temperature to use, between 0 and 2. Default=`0.5`.
- `max_tokens`: int : The maximum number of tokens to generate in the completion. Default=`2000`.
- `context_manager`: ContextManager : Custom ContextManager.


# ğŸ¥ª How it works

As you know, `Completion` endpoint doesn't provide the features for keep and use context.
So, simply, this library send the previous conversation histories with the request text like below.

Prompt on the 1st turn:
```
human:hello
AI:
```
Chatbot returns `Hi, how can I help you?`.

Prompt on the 2nd turn:
```
human:hello
AI:Hi, how can I help you?
human:I'm hungry
AI:
```
Chatbot returns `What would you like to eat?`

:

Prompt on the 6th turn:
```
human:sandwitches
AI:What kind of sandwich would you like?
human:ham&egg        
AI:Would you like that on a white or wheat bread?
human:wheet
AI:Would you like anything else with your ham and egg sandwich on wheat bread?
human:Everything is fine, thank you.
AI:
```
Chatbot returns `Great! Your order has been placed. Enjoy your meal!`


If you change the `username` and `agentname` the senario of conversation changes.
And, setting `chat_description` also make effects on the situations.

Here is an example to simulate the conversation by brother and sister in Japanese.

```python
cc = ContextualChat(
    openai_apikey,
    username="å…„",
    agentname="å¦¹",
    chat_description="ã“ã‚Œã¯å…„ã¨è¦ªã—ã„å¦¹ã¨ã®ä¼šè©±ã§ã™ã€‚ä»²è‰¯ã—ãªã®ã§ä¸å¯§èªã‚’ä½¿ã‚ãšã«è©±ã—ã¦ãã ã•ã„ã€‚"
)
```

Prompt to be sent to OpenAI API is like bellow. `chat_description` is always set to the first line, no matter how many turns of conversation proceed.

```
ã“ã‚Œã¯å…„ã¨è¦ªã—ã„å¦¹ã¨ã®ä¼šè©±ã§ã™ã€‚ä»²è‰¯ã—ãªã®ã§ä¸å¯§èªã‚’ä½¿ã‚ãšã«è©±ã—ã¦ãã ã•ã„ã€‚
å…„:ãŠã¯ã‚ˆãƒ¼
å¦¹:ãŠã¯ã‚ˆãƒ¼ï¼ä»Šæ—¥ã®äºˆå®šã¯ï¼Ÿ
å…„:ã„ã¤ã‚‚é€šã‚Šã€ç‰¹ã«ãªã„ã‚ˆ
å¦¹:ã˜ã‚ƒã‚ã€ä»Šæ—¥ã¯ä¸€ç·’ã«éŠã¼ã†ã‚ˆï¼
å…„:ä½•ã‹ã—ãŸã„ã“ã¨ã‚ã‚‹ï¼Ÿ
å¦¹:ã†ã‚“ã€ä»Šæ—¥ã¯æ˜ ç”»ã‚’è¦‹ã‚ˆã†ï¼
å…„:ã„ã„ã­ã€‚ ã©ã‚“ãªã®ãŒã„ã„ï¼Ÿ
å¦¹:æ€ã„ã¤ã„ãŸï¼ã‚µã‚¹ãƒšãƒ³ã‚¹æ˜ ç”»ãŒã„ã„ã‹ãªï¼
````
